# -*- coding: utf-8 -*-
"""TEXT MINING - SHEIN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rx_LIvgDhMn3iIivloTWelhoX6PgH5nH

**CARGA DE DATOS, IMPORTACIÓN DE LIBRERIAS Y PREPROCESAMIENTO DE DATOS**
"""

!python -m spacy download es_core_news_sm

#Libreria para NPL (procesado natural de lenguaje) en español
import spacy
nlp = spacy.load('es_core_news_sm')

#Otras librerias necesarias 
# Installations
import sys
if 'google.colab' in sys.modules:
    !pip install emoji --upgrade
    !pip install pandas-profiling==2.*
    !pip install plotly==4.*
    !python -m spacy download es_core_web_lg
    !pip install pyldavis
    !pip install gensim
    !pip install nltk
    !pip install chart_studio
    !pip install --upgrade autopep8
    !pip install vaderSentiment
    !pip install datatable
    !pip install unidecode

#Importamos el fichero donde hemos descargado los datos usando el API de Twitter y realizado el pre-procesamiento previo 
df_example=pd.read_csv('df_filtered_final.csv',sep=';')

#Obtenemos las stopwords
stop_words=stopwords.words('spanish')
print(stop_words)

#Visualización de las stopwords más relevantes 
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from nltk.corpus import stopwords

freq_dist = nltk.FreqDist(stop_words)

top_stopwords = freq_dist.most_common(30) # get the 10 most common stopwords

wordcloud_stopwords = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_stopwords))

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_stopwords, interpolation='bilinear')
plt.axis('off')
plt.savefig('stopwords.png', dpi=300)

plt.show()

import warnings
warnings.filterwarnings('ignore') #para ignorar errores y mensajes de aviso que pueda dar Python

#Proceso de lematización usando el diccionario español + ajuste manual de aquellas palabras que detectemos que no han sido correctamente transformadas
nlp = spacy.load('es_core_news_sm')
def lemma_words(text):
    lemmas = []
    doc= nlp(text)
    for token in doc: 
        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):
            lemmas.append(token.lemma_)
    lemmas= [i for i in lemmas if len(i) > 1]
    lemmas = [word for line in lemmas for word in line.split()]
    lemmas=[word for word in lemmas if word not in stop_words]
    lemmas=' '.join(lemmas)
    lemmas=unidecode(lemmas, "utf-8")
    lemmas=re.sub(r"\bllegado\b", "llegar", lemmas)
    lemmas=re.sub(r"\bdigais\b", "decir", lemmas)
    lemmas=re.sub(r"\bpedido\b", "pedir", lemmas)
    lemmas=re.sub(r"\bsuponia\b", "suponer", lemmas)
    lemmas=re.sub(r"\bclasemedar\b", "clase", lemmas)
    lemmas=re.sub(r"\bcolabo\b", "colaborar", lemmas)
    lemmas=re.sub(r"\bfavoritar\b", "favorito", lemmas)
    lemmas=re.sub(r"\bvendio\b", "vender", lemmas)
    lemmas=re.sub(r"\bvendiar\b", "vender", lemmas)
    lemmas=re.sub(r"\bbuscandola\b", "buscar", lemmas)
    lemmas=re.sub(r"\btraigo\b", "traer", lemmas)
    lemmas=re.sub(r"\bcompra\b", "comprar", lemmas)
    lemmas=re.sub(r"\bco\b", "comprar", lemmas)
    lemmas=re.sub(r"\bquiero\b", "querer", lemmas)
    lemmas=re.sub(r"\bhcir\b", "hacer", lemmas)
    lemmas=re.sub(r"\bcutrar\b", "cutre", lemmas)
    lemmas=re.sub(r"\bfranz\b", "franela", lemmas)
    lemmas=re.sub(r"\bfranelar\b", "franela", lemmas)
    lemmas=re.sub(r"\bcuchar\b", "cuchara", lemmas)
    lemmas=re.sub(r"\bshinco\b", "cinco", lemmas)
    lemmas=re.sub(r"\batreve\b", "atrever", lemmas)
    lemmas=re.sub(r"\balmacir\b", "almacen", lemmas)
    lemmas=re.sub(r"\bllego\b", "llegar", lemmas)
    lemmas=re.sub(r"\bcompro\b", "comprar", lemmas)
    lemmas=re.sub(r"\bcompre\b", "comprar", lemmas)
    lemmas=re.sub(r"\bcomprado\b", "comprar", lemmas)
    lemmas=re.sub(r"\bpedi\b", "pedir", lemmas)
    lemmas=re.sub(r"\bhabeis\b", "haber", lemmas)
    lemmas=re.sub(r"\bporfi\b", "favor", lemmas)
    lemmas=re.sub(r"\bafirmamo\b", "afirmar", lemmas)
    lemmas=re.sub(r"\bropar\b", "ropa", lemmas)
    lemmas=re.sub(r"\bpecos\b", "pecosa", lemmas)
    lemmas=re.sub(r"\bcompletamente\b", "completo", lemmas)
    lemmas=re.sub(r"\bdto\b", "descuento", lemmas)
    lemmas=re.sub(r"\boficinar\b", "oficina", lemmas)
    lemmas=re.sub(r"\bropita\b", "ropa", lemmas)
    lemmas=re.sub(r"\bpaquetir\b", "paquete", lemmas)
    lemmas=re.sub(r"\bencanto\b", "encantar", lemmas)
    lemmas=re.sub(r"\bmirad\b", "mirar", lemmas)
    lemmas=re.sub(r"\bmira\b", "mirar", lemmas)
    lemmas=re.sub(r"\bgracia\b", "gracias", lemmas)
    lemmas=re.sub(r"\bdejo\b", "dejar", lemmas)
    lemmas=re.sub(r"\bgastado\b", "gastar", lemmas)
    lemmas=re.sub(r"\bsubido\b", "subir", lemmas)
    lemmas=re.sub(r"\bvuelto\b", "volver", lemmas)
    lemmas=re.sub(r"\bllegao\b", "llegar", lemmas)
    lemmas=re.sub(r"\bpaquetito\b", "paquete", lemmas)
    lemmas=re.sub(r"\bpedir\b", "pedido", lemmas)
    lemmas=re.sub(r"\bmaana\b", "mañana", lemmas)
    lemmas=re.sub(r"\bgusta\b", "gustar", lemmas)
    lemmas=re.sub(r"\bencanta\b", "encantar", lemmas)
    lemmas=re.sub(r"\bspain\b", "españa", lemmas)  # ejemplo de cambios particulares, tenemos que añadir la mayor cantidad posible que nos permita agrupar mejor las palabras
    return lemmas

df_example['lemmas'] = df_example['cleaned_text'].apply(lemma_words)

df_example.dropna(inplace=True)
df_example.head(10)

df_example['lemmas']

"""**UNIGRAMAS, BIGRAMAS Y TRIGRAMAS**"""

dft= df_example['lemmas']
dft = [x for x in dft if str(x) != 'nan']
print(dft)

#Transformación del texto en vectores para obtener la frecuencia de los términos según la metodología TF-IDF
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(1,1))
tfIdf = tfIdfVectorizer_bi.fit_transform(dft)
names=tfIdfVectorizer_bi.get_feature_names_out()
freqs = tfIdf.sum(axis=0).A1
result= dict(zip(names, freqs))

#Obtención de los 30 términos con mayor frecuencia
from operator import itemgetter
i = 0
results_sorted=sorted(result.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Visualización de unigramas más frecuentes
plt.rcParams.update({'font.size': 20})
df_results=pd.DataFrame.from_dict(results_sorted).head(30)
plt.figure(figsize=(12,7))
custom_color = (0.2, 0.4, 0.5)
plt.bar(df_results[0],df_results[1], color=custom_color)
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.savefig('TFIDF.png', dpi=300) # save figure with DPI of 300

plt.title('Histograma de unigramas más frecuentes')

#Igual que antes, creamos los vectores para obtener los bigramas en este caso 
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))

#Visualización de bigramas
plt.rcParams.update({'font.size': 20})
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.figure(figsize=(15,7))
custom_color = (0.2, 0.4, 0.5)
plt.bar(df_results_bi[0],df_results_bi[1], color=custom_color)
plt.xticks(rotation=90)
plt.subplots_adjust(bottom=0.25)
plt.ylabel('TF-IDF Score')
plt.savefig('bigram.png', dpi=300)
#plt.title('Histograma de bigramas más frecuentes')

#Repetimos para los trigramas
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))

#Visualización de trigramas
plt.rcParams.update({'font.size': 18})
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.figure(figsize=(15,7))
custom_color = (0.2, 0.4, 0.5)
plt.bar(df_results_tri[0],df_results_tri[1], color=custom_color)
plt.xticks(rotation=90)
plt.subplots_adjust(bottom=0.25)
plt.ylabel('TF-IDF Score')
plt.savefig('trigram.png', dpi=300)
#plt.title('Histograma de trigramas más frecuentes')

"""**MODELADO DE TÓPICOS**"""

df_model = df_example
print(df_model.head())

#Transformamos el texto en elementos de palabras individuales
def tokenize(text):
    text = str(text)
    tokens = text.split() 
    return tokens


df_model['tokens'] = df_model['lemmas'].apply(tokenize)
print(df_model.head())

#Necesitamos fijar una semilla de reproducibilidad que asegura que los resultados de los procesos aleatorios se pueden reproducir
seed(24)

# Creación de un diccionario a partir de las palabras tokeneizadas previamente
id2word = Dictionary(df_model['tokens'])

#Diccionario que permite eliminar las palabras demasiado extrañas o frecuentes. En este caso hemos puesto que elimine las que aparecen en menos de 2 documentos del corpus y las que aparecen en más del 96% de los documentos del corpus
id2word.filter_extremes(no_below=2, no_above=.95)

#Creamos el corpus 
corpus = [id2word.doc2bow(d) for d in df_model['tokens']]

#Modelo para encontrar el K óptimo (se debe compilar varias veces debido a la aletoriedad del proceso, hasta obtener un resultado consistente)
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics
    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the 
    LDA model with respective number of topics
    """
    coherence_values_topic = []
    model_list_topic = []
    for num_topics in range(start, limit, step):
        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=5)
        #model = LdaMulticore(corpus=corpus, num_topics=k, id2word=dictionary,random_state=100,alpha=a,eta=b) You can try to tune alpha and eta as well
        model_list_topic.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values_topic.append(coherencemodel.get_coherence())

    return model_list_topic, coherence_values_topic

#Calculamos el índice de coherencia para cada modelo generado
model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,
                                                        corpus=corpus,
                                                        texts=df_model['tokens'],
                                                        start=2, limit=15, step=1)

#Cuando obtenemos el modelo óptimo, podemos guardar el archivo con los índices de coherencia obtenidos y directamente abrirlo sin ejecutar las dos últimas celdas
#coherence_values_topic_df = pd.DataFrame(coherence_values_topic) 
#coherence_values_topic_df.to_csv('coherence_values_shein.csv', index=False)
coherence_values_topic_df= pd.read_csv('coherence_values_shein.csv', index_col=False)
coherence_values_topic_df

#Representación gráfica de los modelos obtenidos con sus índices de coherencia 
plt.figure(figsize=(10,7))
plt.plot([2,3,4,5,6,7,8,9,10,11,12,13,14],coherence_values_topic_df) #aqui saca un indice de coherencia por cada número de tópicos y nos quedmaos con el mayor de todos, nos quedamos con el numero de topicos que mayor indice de coherencia nos de. Pero si hay varios valores altos parecidos, nos quedamos con el menor de ellos
plt.xlabel('Número de tópicos')
plt.ylabel('Indice de coherencia')

#Representación del K óptimo
plt.rcParams.update({'font.size': 22})
import matplotlib.path as mpath
plt.figure(figsize=(12,7))
estrella = mpath.Path.unit_regular_star(6)
circulo = mpath.Path.unit_circle()
verts = np.concatenate([circulo.vertices, estrella.vertices[::-1, ...]])
codes = np.concatenate([circulo.codes, estrella.codes])
cut_star = mpath.Path(verts, codes)
plt.plot([2,3,4,5,6,7,8,9,10,11,12,13,14],coherence_values_topic_df, '--r', marker="o", markersize=10, fillstyle='none')
plt.axvline(x=5, color='b', linestyle='--') #aqui ponemos según la figura anterior  que K queremos indicar
plt.plot(5, 0.345 , '--r', marker=cut_star, markersize=18) #aqui ponemos el K que nos salga mayor y su valor correspondiente
plt.xlabel('Número de tópicos')
plt.xticks(np.arange(2, 14, step=1))
plt.ylabel('Indice de coherencia')

#Guardamos el modelo 
#model_k5.save("mode_topics_shein_k5_final.model") 

#Ejecutamos el modelo cargándolo directamente
model_k5=LdaMulticore.load("mode_topics_shein_k5_final.model")

# Mostramos su índice de coherencia
coherence_model_k5 = CoherenceModel(model=model_k5, texts=df_model['tokens'], 
                                   dictionary=id2word, coherence='c_v')
coherence_model_k5 = coherence_model_k5.get_coherence()
print('\nCoherence Score: ', coherence_model_k5)

# Mostramos las palabras clave por cada uno de los tópicos obtenidos
print(model_k5.print_topics())
doc_lda = model_k5[corpus]

words = [re.findall(r'"([^"]*)"',t[1]) for t in model_k5.print_topics()]

topics = [' '.join(t[0:10]) for t in words]

for id, t in enumerate(topics): 
    print(f"------ Topic {id} ------")
    print(t, end="\n\n")

#Representamos uan figura con la distancia intertópica. Cuanto más grandes los círculos más relevantes para el corpus y buscamos que estén cuanto más alejados para que sean interpretables y cada tópico contenga elemenentos diferenciales
pyLDAvis.enable_notebook()
gensimvis.prepare(model_k5, corpus, id2word)

#Mostramos el peso de cada palabra dentro del tópico 
model_k5.print_topics()

def document_to_lda_features(model_k5,document):
  topic_importance=np.array(model_k5.get_document_topics(document, minimum_probability=0))
  return topic_importance[:,1]

df_model['lda_features']=list(map(lambda doc: document_to_lda_features(model_k5,doc), corpus))

#NO ME QUEDA CLARO POR QUE VOLVEMOS A TOKENEIZAR

def topic_important(item_score):
    score=np.argmax(item_score, axis=0)
    return score


df_model['topic_dominant'] = df_model['lda_features'].apply(topic_important)
df_model

#Visualizamos la distribución de tópicos en el corpus 
ax=df_model["topic_dominant"].value_counts().sort_index().plot(kind='bar')
plt.ylabel('Frecuencia Absoluta')
plt.title('Distribución de tópicos en el corpus textual')
plt.show()

#Obtención de bigramas por tópico

topic_1 = df_model[df_model['topic_dominant']==0]  #tenemos que copiar o ejecutar sucesivamente este codigo por cada tópico que tengamos, cambiando el número (en este caso del 0 al 4)
dft=topic_1['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2)) 
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi_1= dict(zip(names_bi, freqs_bi))
# We will then obtain the first 30 most popular words in the TF-IDF####
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi_1.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Obtención de trigramas por tópico

topic_1 = df_model[df_model['topic_dominant']==0]  #tenemos que copiar o ejecutar sucesivamente este codigo por cada tópico que tengamos, cambiando el número (en este caso del 0 al 4)
dft=topic_1['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(3,3)) 
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi_1= dict(zip(names_bi, freqs_bi))
# We will then obtain the first 30 most popular words in the TF-IDF####
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi_1.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

"""**ANÁLISIS DE SENTIMIENTO**"""

#Cargamos el fichero, donde hayamos realizado el pre-procesamiento correspondiente para el análisis de sentimiento (que es diferente al que se lleva a cabo para el modelado de tópicos)
df_sentiment = pd.read_csv('df_filtered_sentiment.csv', sep=";")

df_sentiment ['cleaned_text_sent']

df_model_sent=df_model.merge(df_sentiment, how='left')
df_model_sent=df_model_sent.drop_duplicates(subset=["cleaned_text_sent","author_id","fecha","lang","retweet_number","reply_number","like_number","year"]).reset_index(drop=True)
df_model_sent

#Tokeneizamos los datos 
def tokenize(text):
    text = str(text)
    tokens = text.split() 
    return tokens
df_model_sent ['tokens'] = df_model_sent ['cleaned_text_sent'].apply(tokenize)
print(df_sentiment.head())

df_model_sent['cleaned_text_sent']

df_model_sent=df_model_sent.dropna(subset=['cleaned_text_sent']).reset_index(drop=True)

import emoji
df_model_sent['cleaned_text_sent_emoji'] = df_model_sent['cleaned_text_sent'].apply(lambda s: emoji.replace_emoji(s, ''))
df_model_sent

#Obtenemos las puntuaciones compuestas de sentimiento
def tweet_scores(data_frame): 
  
   
    sid_obj = SentimentIntensityAnalyzer()
    scores = []
    n = data_frame.shape[0]
    for i in range(n):
      sentence = data_frame.iloc[i]
      sentiment_dict = sid_obj.polarity_scores(sentence)
      compound = sentiment_dict['compound']
      scores.append(compound)
    return scores

df_model_sent['scores']=tweet_scores(df_model_sent['cleaned_text_sent_emoji'])

df_model_sent['scores'].describe()

import datetime as dt

df_model_sent['date'] = pd.to_datetime(dict(year=df_model_sent.year, month=df_model_sent.month, day=df_model_sent.day))
df_model_sent

#Visualizamos la puntuación de sentimiento a lo largo de tiempo 


plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="darkblue")
plt.ylim([-6,3])
plt.xlabel('Fecha',fontsize=18)
plt.ylabel('Puntuaciones',fontsize=18)
plt.tick_params(labelsize=16)
plt.grid(color='lightgray', linestyle='--')
plt.show()

#LLevamos a cabo gráficas por cada uno de los tópicos para analizar de forma aislada el sentimiento

topic_1_sentiment = df_model_sent[df_model_sent['topic_dominant']==0] #igual que con los bigramas, vamos cambiando el número en cada iteración por cada uno de los tópicos
topic_1_sentiment = topic_1_sentiment[topic_1_sentiment.date.notnull()]
df_bygroup=pd.DataFrame(topic_1_sentiment.groupby(['date'])['scores'].agg('sum'))
df_bygroup['date']=df_bygroup.index
df_bygroup.to_excel('topico1.xlsx', index=False)
df_bygroup

#Gráfica
plt.figure(figsize=(15,7))
plt.rcParams['agg.path.chunksize'] = 10000
plt.plot(df_bygroup['date'],df_bygroup['scores'],color="red")
plt.ylim([-2.5,2])
plt.xlabel('Fecha',fontsize=18)
plt.ylabel('Puntuaciones',fontsize=18)
plt.tick_params(labelsize=16)
plt.grid(color='lightgray', linestyle='--')
plt.show()